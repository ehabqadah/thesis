
\subsection{Distributed Online Learning}

\par In recent years, there have been many research efforts on the problem of distributed online learning over multiple data streams \cite{tekin2014distributed,yan2013distributed,canzian2015ensemble,zhang2014fast,dekel2012optimal,kamp2014communication}.   In contrast to the centralized learning approach in which all records of the streams are processed at a single central machine, the data streams are processed in a distributed fashion on $k$ local learners. Each of which learner processes a single stream and use a local learning algorithm to provide a real-time prediction based service like Classification \cite{canzian2015ensemble}.  

\par However, this distributed setting requires the learners to exchange the parameters of underlaying learning algorithm to construct a strong global model, in order to preserve a similar predictive performance to the centralized setting \cite{kamp2014communication}. 

\par Different communication schemes between the distributed learners have been proposed in the literature. For instance, ~\citet{dekel2012optimal} have proposed a distributed online mini-batch prediction approach over multiple data streams. Their approach is based on a static synchronization method. The distributed learners/predictors periodically communicate  their local models with a central coordinator unit after consuming a fixed number of input samples/events (i.e., batch size $b$), in order to  create a global model  and share it among all local learners. 

\par This work has been extended in \cite{kamp2014communication} by introducing a dynamic synchronization scheme that reduces the required communication to the exchange of information between the learners. The proposed protocol relays on a dynamic synchronization operator that controls when the local learners communicate their models, which is based on only synchronizing the local models of the learners if they diverge from a reference model. 
 
\par This protocol was introduced for linear models, and has been extended to handle kernelized online learning models \cite{kamp2016communication}. In this work, we consider the event patterns prediction models over multiple event streams as learning algorithms, and we introduce to employ the communication-efficient distributed online learning protocol \cite{kamp2014communication} to synchronize their parameters as illustrated in Section ~\ref{sec:proposed_approach}. 
