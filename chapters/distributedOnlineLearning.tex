
\subsection{Distributed Online Learning}

\par In recent years, there have been many research efforts on the problem of distributed online learning  \cite{tekin2014distributed,yan2013distributed,xiao2010dual,dekel2012optimal,kamp2014communication}.  In contrast to the centralized learning approach, the large data sets are divided into partitions, and then processed in a distributed fashion on $k$ machines/learners. However, this setting requires to aggregate the parameters of underlaying learning algorithm among the learners to construct a strong global model. 
\par For instance, a distributed online mini-batch prediction approach over multiple data streams has been proposed in \cite{dekel2012optimal}. This approach is based on a static synchronization method. The distributed learners/predictors periodically communicate  their local models with a central coordinator unit after consuming a fixed number of input samples/events (i.e., batch size $b$), in order to  create a global model parameters and share them between all learners. This work has been extended in \cite{kamp2014communication} by introducing  dynamic synchronization scheme that reduces the required communication overhead. It can do so by making the local learners communicate their models only if they diverge from a reference model. This protocol was introduced for linear models, and has been extended to handle kernelized online learning models \cite{kamp2016communication}.  

\par In this work, we consider the event patterns prediction models over multiple event streams as learning algorithms, and we introduce to employ the communication-efficient distributed online learning protocol \cite{kamp2014communication} to synchronize their parameters as illustrated in Section ~\ref{sec:proposed_approach}. 
