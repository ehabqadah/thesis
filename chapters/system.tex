\chapter{Approach and Theoretical Analysis}
\label{sec:system}
\section{Pattern Prediction Over an Event Stream}
%refine the stream and event pattern %
%define the pattern we deal formally%
% include base line 1 batch size in experemintal results%

%In this section we give a brief introduction to an approach to forecasting
%based on the state space model.


This chapter introduces the problem that we address in this thesis, including the formal definition of the pattern prediction over single event stream and multiple streams. First, we introduce the base model for pattern prediction, and our proposed approach to leverage the distributed online protocol to enable knowledge sharing between prediction models over multiple event streams. Furthermore, we provide an analysis of the theoretical efficiency of distributing the pattern prediction models.
We follow the general notation and terminology of \cite{schultz2009distributed,luckham2008power,alevizos2015complex,zhou_pattern_2015,kamp2014communication} to formalize the problem we tackle and our solution approach.

\subsection{Problem Formulation}

We first define the input event and the stream of events as follows:  
\begin{definition}
	Each event is defined as a tuple of attributes $e_i = (id,type,\tau,a_1,a_2.....,a_n)$, where $type$ is the event type attribute that takes a value from a set of finite event types/symbols $\Sigma$, $\tau$ represents the time when the event tuple was created,  the  $a_1,a_2,...,a_n$ are spatial or other contextual features (e.g., speed); these features are varying from one application domain to another. The attribute $id$ is a unique identifier that connects the event tuple to an associated domain object.
\end{definition}

\begin{definition}
A stream $s=\langle e_1,e_3,...,e_t,...\rangle$  is a time-ordered sequence of events.
\end{definition}


\par A user-defined pattern $\mathcal{P}$ is given in the form of a regular expression over  a set of event types $\Sigma$ (i.e., alphabet). Let a word over $\Sigma$ be a sequence of event types, and the set of words over $\Sigma$ be a language  $L$. Then $L(\mathcal{P})$ is the regular language defined by the regular expression ($\mathcal{P}$) \cite{hopcroft2006automata,nuel_pattern_2008,alevizos2017event},  where a regular expression is an empty word or an event type $\in$ $\Sigma$, or  defined using the following operators over regular expressions:
 %formlize them more and try to connect regexp with event recognization operators%
\begin{itemize}[noitemsep]
	\item \textit{sequence} that represents a concatenation of two regular expressions languages 
	\item \textit{disjunction} i.e., union, which is the language that its words belong to one of the languages of the two regular expressions 
	\item \textit{iteration} operation to define the set of all possible concatenation over a regular expression
\end{itemize}

More formally, a pattern is given through the following grammar:
\begin{definition}
$\mathcal{P} := E\ |\ \mathcal{P}_{1} ; \mathcal{P}_{2}\ | \mathcal{P}_{1} \vee \mathcal{P}_{2}\ |\ \mathcal{P}_{1}^{*}  $, where $E \in \Sigma$ is a constant event type. $;$ stands for sequence, $\vee$ for disjunction and $*$ for $\mathit{Kleene}-*$.
The pattern $\mathcal{P} := E$ is matched by reading an event $e_i$ iff $e_{i}.type = E$.
The other cases are matched as in standard automata theory.
\end{definition}


The problem setting can be summarized as follows: given a stream $s$ of low-level events and a pattern $\mathcal{P}$, 
the goal is to estimate at each new event arrival the number of future events
that we will need to wait for until the pattern is completed (and therefore a full match is detected).

\subsection{Base Model}
\label{sec:Event-Forecasting-PMC}

\par In this thesis, we use the Event Forecasting with Pattern Markov Chains approach \cite{alevizos2017event} as the base to construct a pattern prediction model over an event stream. In next, we describe the details of this approach. We first provide an overview of the Patten Markov Chain framework. We then describe how this framework is used to build a pattern prediction model.  

%first assuming that only a single stream is consumed
%and then adjusting for the case of multiple streams.

\subsubsection*{Construction of Pattern Markov Chains (PMCs)}

~\citet{alevizos2017event} proposed to employ the Pattern Markov Chain (PMC) \cite{nuel_pattern_2008}to build an online prediction associated with a pattern over an event stream. The algorithm of constructing a PMC associated with a pattern $\mathcal{P}$ over a stream of events $s$ consists of the following steps:

\begin{itemize}[noitemsep]
	\item Build a deterministic finite automata (DFA) that accepts the regular expression $\Sigma^{*};\mathcal{P}$. We define the the DFA as following:
	
	\begin{definition}[\cite{hopcroft2006automata}]
	 $(\Sigma,Q,s,F,\delta)$ is a deterministic finite automaton (DFA)  where  $\Sigma$ a finite alphabet of event types, and $Q$ is a finite set of states, $s \in Q$ a start state and $F \subset Q$ represents all final states. $\delta: Q \times \Sigma \rightarrow Q$ is a transition function from a state to another state given an input event type, which is defined as recursive function $\delta(q,e_{1}e_{2}\ldots e_{d})=\delta(\delta(q,e_{1}e_{2}...e_{d-1}),e_{d})$. If $\delta(s,w) \in F$ of a word $w=e_{1}e_{2}\ldots e_{d} \in \Sigma^{*}$, then the word $w$ is said to be accepted by the DFA. In addition,  $\delta(q)^{-m}$ is the set of words of size $m$ defined as $\{w \in \Sigma^{m} \mid \exists q \in Q,\ \delta(p,w)=q \}$
\end{definition}
	
	First, the regular expression $\Sigma^{*};\mathcal{P}$ is converted to non-deterministic finite automata (NFA), then an equivalent DFA of the constructed NFA is computed using the subset construction \cite{hopcroft2006automata,alevizos2017event}. And the events stream $s$ is considered as the input word of the DFA. 
 Figure ~\ref{fig:dfatcc} shows an associated DFA of a sequential pattern $\mathcal{P}=a ; c ; c$ over an alphabet $\Sigma=\{a,b,c\}$.

Furthermore, if $\forall q \in Q$ of a DFA the all $\delta(q)^{-m}$ are sets of cardinality 1, then the DFA is called \textit{m-unambiguous}. ~\citet{nuel_pattern_2008} proposed a procedure to transform any DFA to an \textit{m-unambiguous} DFA, which is achieved by incrementally duplicate all states have m-ambiguity (i.e., $\vert\delta(q)^{-m}\vert > 1$).   

%(note that the DFA has no dead states since we need to handle streams and not strings).

\item Assume that the input events stream $s=\langle e_1,e_3,...,e_t,...\rangle$ is a $m$-order Markov sequence, where $m \geq 1$.  Given a constructed \textit{m-unambiguous} DFA for the pattern $\mathcal{P}$, ~\citet{nuel_pattern_2008} showed that the sequence of states of the DFA that generated by consuming the input events stream $s$ is a first-order Markov chain, which is represented by $\langle q_{0},q_{1},...,q_{t},...\rangle$, where $q_{0}=s$ and $q_{t}=\delta(q_{t-1},e_{t})$. We denote by \pmcmr  the derived Markov chain associated with a pattern $\mathcal{P}$ that is called a Pattern Markov Chain (PMC) of order $m$  \cite{nuel_pattern_2008}. In other words, we perform a direct mapping of the states of the DFA to states of a Markov chain and the transitions of the DFA to transitions of the Markov chain.Thus, the terms \textit{m-unambiguous} DFA of a pattern and the corresponding \pmcmr  we will be used interchangeably. 

\par Furthermore, the \pmcmr is characterized by $\vert Q \vert \times \vert Q \vert$ transition probability matrix $\Pi$ where $Q$ is again the set of states of the  \textit{m-unambiguous} DFA. Figure \ref{fig:mctcc1} depicts the PMC of order 1 for the generated DFA of Figure \ref{fig:dfatcc}.

%\par The next step is to derive a Markov chain that will be able to provide a probabilistic description of the DFA's run-time behavior.
%\par Towards this goal, we use Pattern Markov Chains, as was proposed in \cite{nuel_pattern_2008}.
%, it can be shown that there is a direct mapping of the states of the DFA to states of a Markov chain and the transitions of the DFA to transitions of the Markov chain.
%\par The transition probabilities of the Markov chain are the occurrence probabilities of the various event types.
%On the other hand, if the occurrence probabilities of the events are dependent on some of the previous events  seen in the stream (i.e., the stream is generated by an $m^{th}$ order Markov process), we might need to perform a more complex transformation 
%(see \cite{nuel_pattern_2008} for details)
%in order to obtain a ``proper'' Markov chain.
%The transition probabilities are then conditional probabilities on the event types.
%In any case,
%we call such a derived Markov chain a Pattern Markov Chain (PMC) of order $m$
%and denote by \pmcmr , where $\mathcal{P}$ is the initial pattern and $m$ the assumed order.
 
\end{itemize}


\begin{figure}[!ht]
\begin{centering}

\includegraphics[width=0.35\textwidth]{./chapters/figures/forecasting/dfasr.pdf}
\label{fig:dfatcc}

\hfill

\includegraphics[width=0.35\textwidth]{./chapters/figures/forecasting/pmcr1.pdf}
\label{fig:mctcc1}

%\hfill
\caption{DFA and PMC for $\mathcal{P}=a ; c ; c$,  $\Sigma=\{a,b,c\}$, and order $m=1$  \cite{alevizos2017event}.}
\label{fig:dfa_mc_example}
\end{centering}
\end{figure}
%\end{comment}

\subsubsection*{Constructing Pattern Prediction Model}
\label{sec:pmc_prediction}

~\citet{alevizos2017event} proposed to use the constructed \pmcmr to build a probabilistic prediction model that describes the DFA's run-time behavior. The method is based on calculating the \textit{waiting-time} distributions. Given a specific state of the \pmcmr, a \textit{waiting-time} distribution provides the probability of reaching a set of absorbing states in $n$ transition from current state. So by mapping the final states of the DFA to absorbing states of the \pmcmr by adding self-loops with probabilities equal to $1.0$. Therefore, we can calculate the probability of reaching a final state in $n$ transitions,  which means predicting a full match of the defined pattern $\mathcal{P}$.

\par We denote by $W_{\mathcal{P}}(q)$  the waiting-time random variable that represents the
number of transitions until from a current state $q$ of DFA to reach a final state\cite{alevizos2017event}, which is given by 

\begin{equation*}
W_{\mathcal{P}}(q)=inf\{n: q_{0},q_{1},...,q_{n}, q_{0}=q, q \in Q \backslash F, q_{n} \in F\}
\end{equation*}

However, the \textit{waiting-time} distribution of the $W_{\mathcal{P}}(q)$ random variable can be computed based on the transition probability matrix $\Pi$ of the \pmcmr, where it has $h$ non-final states and $d$ final states (absorbing states) \cite{alevizos2017event}, then distribution is calculated by the following equation 

\begin{equation*}
P(W_{\mathcal{P}}(q)=n)=\boldsymbol{\xi_{q}}\boldsymbol{N}^{n-1}(\boldsymbol{I}-\boldsymbol{N})\boldsymbol{1}
\end{equation*}
where $\boldsymbol{N}$ is $h \times h$ matrix that obtained by re-arranging the transition matrix $\Pi$ to include transitions between the non-final states of the DFA, $\boldsymbol{I}$ is an identity matrix of size $d \times d$, and  $\boldsymbol{1}$ is $h \times 1$ vector of ones. The $\boldsymbol{\xi_{q}}$ is $1 \times h$ row of elements that contains zeros except $1.0$ in the cell corresponding to $q$. 
\par Finally, we provide the prediction reports in the form of intervals i.e.,  $I=(\mathit{start},\mathit{end})$. Which means that the DFA is expected to reach a final state in after future transitions between $\mathit{start}$ and $\mathit{end}$ with probability at least some constant threshold $\theta_{fc}$ that defined by the user. 
%ADD equation of I as in %

These intervals are estimated by a single-pass algorithm that scans a waiting-time distribution and finds the smallest (in terms of length) interval that exceeds this threshold. 
An example is shown in Figure \ref{fig:wtdfas},
where the DFA in Figure \ref{fig:dfa1} is in state $1$,
the \textit{waiting-time} distributions for all of its non-final states are shown in Figure \ref{fig:wt1}
and the distribution, along with the prediction interval, for state $1$ are depicted in green.
\begin{figure}[!ht]
\begin{centering}

\includegraphics[width=0.19\textwidth]{./chapters/figures/forecasting/dfa1.pdf}
\label{fig:dfa1}


\includegraphics[width=0.28\textwidth]{./chapters/figures/forecasting/wt1.pdf}
\label{fig:wt1}

\caption{Example of how prediction intervals are produced. 
$\mathcal{P}=a ; b ; b ; b$, $\Sigma=\{a,b\}$, $m=1$, $\theta_{\mathit{fc}}=0.5$      \cite{alevizos2017event}.}
\label{fig:wtdfas}
\end{centering}
\end{figure}

\par The proposed method assumes that the transition probability matrix $\Pi$ is available to build the prediction intervals. However, this is not true in the real-world applications.
Therefore, it is essential to learn the values of the \pmcmr's transition probability matrix in order apply this method. One common way, is to use the maximum-likelihood estimator to learn the transition probabilities as illustrated in Section ~\ref{sec:theoretical}. This model is performing the learning over a single event stream $s$ that might requires a large amount of time until convergence to a sufficiently good model. In this thesis, we present a technique to distribute the learning of  transition probability matrix over multiple input event streams.

\section{Pattern Prediction over Multiple Event Streams}

\subsection{Problem Formulation Extension}

\par In this section, we extend the problem of pattern prediction over an event stream to consider the setting where there are multiple input event stream: 
Let $O = \{ o_1, ..., o_k\}$ be a set of \emph{$K$} objects/entities (e.g., moving objects), each of which is generating an event stream $s_i$, hence, we have a set of real-time input event streams $S = \{ s_1, ..., s_k\}$.
%ADD comment about the streams distribution%

\par Let $\mathcal{P}$ be a user-defined pattern given in the form of regular expression, which is monitored in every stream $s_i \in S$. Then, we have a system consisting of \emph{$K$} distributed predictor (i.e., learner) nodes $n_1,n_2...,n_k$, each of which receives an input event stream $s_i\in S$ associated with an object $o_i \in O$.

\par  The goal is to provide online predictions about the completion of the pattern $\mathcal{P}$ within each stream $s_i$. Toward this goal, each node maintains a local prediction model $f_i$ associated the user-defined pattern $\mathcal{P}$. Then for each new arriving event tuple  $e_t \in s_i$, the $f_i$ model provides an online prediction interval about the future full match of the pattern $\mathcal{P}$.

\par In summary, we have multiple running instances of an online prediction algorithm on distributed nodes for multiple input event streams. For example, massive streams of events that describe trajectories of moving vessels in the context of maritime surveillance, where there is one predictor node for each vessel's event stream.
  
%
%The defined pattern $\mathcal{P}$ is monitored over each event stream $s_i$  by a  predictor nodes  $n_i$  that maintains a local prediction model $f_i$, where there is one node for each vessel's event stream.  The prediction model $f_i$ gives the ability to provide an online predictions about when the pattern will be completed in the form of an expected number of future events before a full match does occur.

\subsection{The Proposed Approach}
\label{sec:proposed_approach}
\par We design and develop a scalable and distributed pattern prediction system over a massive input event streams. As the base prediction model, we use the PMC forecasting method \cite{alevizos2017event}, where there is a \pmcmr model associated with each input event stream $s_i$. Thus, the prediction model $f_i$ on a predictor node is represented by \pmcmr and its associated DFA.

\par We propose to enable the information exchange among the distributed predictors of the input event streams, by adapting a distributed online prediction protocol \cite{kamp2014communication} to synchronize the parameters of prediction models, which are represented by the transitions probabilities matrix of the \pmcmr predictors. This protocol provides a communication-efficient dynamic synchronization scheme based on a periodic static scheme. In next, we  describe the details of adapting this protocol with pattern prediction models.


\par Algorithm~\ref{algonline:dol} presents the distributed online prediction protocol by dynamic model synchronization on both the predictor nodes and the coordinator. We refer to the PMC's transition matrix $\boldsymbol{\Pi}_i$ on predictor node $n_i$ by $w_i$. That is, when a predictor $n_i:\ i \in[k]$ observes an event $e_j$ it revises its internal model state (i.e., $f_i$) and provides a prediction report. Then it checks the local conditions  (batch size $b$ and local model divergence from a reference model $w_r$) to decide whether there is a need to synchronize its local model with the coordinator [or not].  $w_r$ is maintained in the predictor node as a copy of the last computed aggregated model $\hat{w}$ from the previous full synchronization step, which is shared between all local predictors/learners. By monitoring the local condition $\|w_i - w_r\|^2 > \Delta$ on all local predictors, we have a guarantee that if none of the local conditions is violated, the divergence (i.e., variance of local models $\delta(w)=\frac{1}{k} \sum_{j=1}^{k}\|w_i - \hat{w}\|^2$) does not exceed the threshold $\Delta$ \cite{kamp2014communication}. 

\par On the other hand, the coordinator receives the prediction models from the predictor nodes that requested for model synchronization (violation). Then it tries to keep incrementally querying other nodes for their local prediction models until reaching out all nodes, or the variance of the aggregated model $\hat{w}$ that is computed from the already received models less or equal than the divergence threshold  $\Delta$. Finally, the aggregated model $\hat{w}$ is sent back to the predictor nodes that sent their models after the violation or have been queried by the coordinator.

\begin{algorithm}[h]
	\caption{Communication-efficient Distributed Online Learning \cite{kamp2014communication}.} 
	\begin{algorithmic}[1] 
	
		\Statex \Indm  \textbf{Predictor} node $n_i$: at observing event $e_j$
		\Statex \Indp update the parameters of the local prediction model $w_i$ and provide a prediction interval $I$ \; 

		\Statex \If {$j\mod b = 0\ and\ \|w_i - w_r\|^2 > \Delta$}  
		\Statex send  $w_i$ to the Coordinator (violation) \;
		\Statex \Indm \textbf{Coordinator}:
		\Statex \Indp receive local models with violation 
		 $B=\{w_i\}_{i=1}^m$ \;
	
	
		\Statex \While{$|B| \neq k $ and $\frac{1}{|B|} \ \sum_{w_i\in  \Pi}\|w_i - \hat{w}\|^2 > \Delta$}{
			
			 \Statex  \hspace{\algorithmicindent} add other nodes have not reported violation for \Statex \hspace{\algorithmicindent} their models $ B \gets \{w_l : f_l \notin B\ and\ l \in [k]\}$    \;
			\Statex  \hspace{\algorithmicindent} receive models from nodes in $B$\;
	}
        \Statex
		\Statex compute a new global model $\hat{w}$ \;
		\Statex send $\hat{w}$ to all the predictors in $B$ and set $f_{1}\dots f_{m}=\hat{w} $\; 
		\Statex \If {$|B| = k$}{
		\Statex  \hspace{\algorithmicindent} set a new reference model $w_r	\gets \hat{w}$ \; }
	
	\end{algorithmic}
	\label{algonline:dol}
\end{algorithm}


\par  We use this protocol for the pattern prediction model, which is internally based on the \pmcmr. This allows the distributed \pmcmr\ predictors for multiple event streams to  synchronize their models (i.e., the transition probability matrix of each predictor) within the system in a communication-efficient manner. 



\par We propose a \textit{synchronization operation} for the parameters of the models ($w_i=\boldsymbol{\Pi}_i :i \in[k]$) of the $k$ distributed PMC predictors. The operation is based on distributing the maximum-likelihood estimation \cite{anderson1957statistical} for the transition probabilities of the underlying \pmcmr\ models described by: 
\begin{equation}
\label{eq:dis_pi_estim}
\hat{\pi}_{i,j}=\frac{\sum_{k \in K} n_{k,i,j}}{\sum_{k \in K} \sum_{l \in L} n_{k,i,l}}
\end{equation}

\par Moreover, we measure the divergence of the local models from the reference model  $\|w_k - w_r\|^2$ by calculating the sum of square difference between the transition probabilities  $\boldsymbol{\Pi}_i$ and  $\boldsymbol{\Pi}_r$:
\begin{equation*}
\label{eq:dis_pi_varinace}
\|w_k - w_r\|^2=\sum_{i,j} (\hat{\pi}_k{i,j} -\hat{\pi}_r{i,j})^2
\end{equation*}


\section{Analysis of Proposed Approach}

\par Generally, our approach relies on enabling the collaborative learning among the distributed predictors. Each predictor node receives a stream of events related to a distinct moving object, and the central coordinator is responsible of synchronizing their prediction models using the \textit{synchronization operation}. Moreover, the predictors they only need to share the parameters of their models. 

\par In addition, our approach relies on enabling the collaborative learning between the prediction models of  the input event streams. By doing so, we assume that the underlying event streams belong to the same  distribution and share the same behavior (e.g., mobility patterns). We claim this assumption is reasonable in many application domains: for instance, in the context of maritime surveillance, vessels travel through standard routes, defined by the International Maritime Organization (IMO). Additionally, vessels have similar mobility patterns in specific areas such as moving with low speed and multiple turns near the ports \cite{pallotta2013vessel,liu2014knowledge}. That allows our system to construct a coherent global prediction model dynamically for all input event streams based on merging their local prediction models.

% may add it to conclusion 
%\par By enabling collaborative learning our approach is imposing an acceleration of learning of the underlying prediction models with less training data, in addition, it provides an improvement of the predictive performance compared to the no-distributed  version of event forecasting with Pattern Markov Chains system. 

\input{chapters/theoretical}


\begin{figure}[ht]
\begin{center}
	\begin{tikzpicture}[scale=0.2]
	\tikzstyle{every node}+=[inner sep=0pt]
	\draw [black] (3.9,-24.1) circle (3);
	\draw (3.9,-24.1) node {$0$};
	\draw [black] (19.8,-9) circle (3);
	\draw (19.8,-9) node {$1$};
	\draw [black] (30,-19.6) circle (3);
	\draw (30,-19.6) node {$2$};
	\draw [black] (18.5,-36.1) circle (3);
	\draw (18.5,-36.1) node {$3$};
	\draw [black] (35.8,-39.1) circle (3);
	\draw (35.8,-39.1) node {$4$};
	\draw [black] (43.4,-13.2) circle (3);
	\draw (43.4,-13.2) node {$5$};
	\draw [black] (46.6,-27.4) circle (3);
	\draw (46.6,-27.4) node {$6$};
	\draw [black] (46.6,-27.4) circle (2.4);
	\draw [black] (33.919,-41.432) arc (-43.70358:-186.66407:17.839);
	\fill [black] (33.92,-41.43) -- (33,-41.67) -- (33.73,-42.36);
	\draw (11.22,-45.77) node [below] {$d\mbox{ }\mbox{ }\mbox{ }\mbox{ }\mbox{ }$};
	\draw [black] (6.22,-26) -- (16.18,-34.2);
	\fill [black] (16.18,-34.2) -- (15.88,-33.3) -- (15.25,-34.07);
	\draw (10.25,-30.59) node [below] {$c$};
	\draw [black] (5.821,-21.8) arc (135.53989:64.02493:18.753);
	\fill [black] (27.42,-18.08) -- (26.92,-17.28) -- (26.48,-18.18);
	\draw (15.1,-15.77) node [above] {$b\mbox{ }\mbox{ }\mbox{ }\mbox{ }$};
	\draw [black] (4.28,-21.129) arc (167.20329:99.84016:15.584);
	\fill [black] (16.81,-9.23) -- (15.94,-8.87) -- (16.11,-9.86);
	\draw (7.78,-12.8) node [above] {$a$};
	\draw [black] (17.19,-7.545) arc (268.59189:-19.40811:2.25);
	\draw (15.02,-2.94) node [above] {$a$};
	\fill [black] (19.37,-6.04) -- (19.89,-5.26) -- (18.89,-5.23);
	\draw [black] (32.505,-17.971) arc (150.77527:-137.22473:2.25);
	\draw (37.37,-18.68) node [right] {$b$};
	\fill [black] (32.82,-20.6) -- (33.27,-21.42) -- (33.76,-20.55);
	\draw [black] (21.479,-35.863) arc (122.28011:-165.71989:2.25);
	\draw (25.52,-39.48) node [right] {$c$};
	\fill [black] (20.5,-38.32) -- (20.5,-39.26) -- (21.35,-38.73);
	\draw [black] (17.251,-33.374) arc (-158.59999:-206.89281:26.653);
	\fill [black] (17.25,-33.37) -- (17.42,-32.45) -- (16.49,-32.81);
	\draw (14.87,-22.35) node [left] {$\mbox{ }\mbox{ }\mbox{ }\mbox{ }\mbox{ }\mbox{ }\mbox{ }\mbox{ }c$};
	\draw [black] (22.205,-7.215) arc (120.91152:38.90637:15.136);
	\fill [black] (41.76,-10.69) -- (41.65,-9.76) -- (40.87,-10.39);
	\draw (33.11,-4.71) node [above] {$d$};
	\draw [black] (22.741,-8.501) arc (88.91183:-1.11524:8.025);
	\fill [black] (22.74,-8.5) -- (23.53,-9.02) -- (23.55,-8.02);
	\draw (28.9,-9.49) node [right] {$a$};
	\draw [black] (19.478,-33.265) arc (158.24167:132.00767:31.599);
	\fill [black] (19.48,-33.27) -- (20.24,-32.71) -- (19.31,-32.34);
	\draw (22.31,-25.55) node [left] {$c$};
	\draw [black] (33.704,-36.958) arc (-140.13045:-186.74081:19.012);
	\fill [black] (33.7,-36.96) -- (33.57,-36.02) -- (32.81,-36.66);
	\draw (29.3,-30.79) node [left] {$\mbox{ }\mbox{ }d$};
	\draw [black] (35.581,-42.078) arc (-13.66199:-186.01371:9.092);
	\fill [black] (35.58,-42.08) -- (34.91,-42.74) -- (35.88,-42.97);
	\draw (24.73,-49.48) node [below] {$d$};
	\draw [black] (19.971,-11.995) arc (2.52337:-8.01617:115.218);
	\fill [black] (19.97,-12) -- (19.51,-12.82) -- (20.51,-12.77);
	\draw (20.52,-22.61) node [right] {$a$};
	\draw [black] (21.88,-11.16) -- (27.92,-17.44);
	\fill [black] (27.92,-17.44) -- (27.73,-16.52) -- (27,-17.21);
	\draw (25.43,-12.83) node [right] {$b$};
	\draw [black] (28.674,-22.291) arc (-27.74002:-42.01064:57.083);
	\fill [black] (28.67,-22.29) -- (27.86,-22.77) -- (28.74,-23.23);
	\draw (25.58,-29.72) node [right] {$b$};
	\draw [black] (22.741,-8.416) arc (98.28712:61.53076:29.155);
	\fill [black] (22.74,-8.42) -- (23.6,-8.8) -- (23.46,-7.81);
	\draw (33.11,-7.86) node [above] {$a\mbox{ }\mbox{ }\mbox{ }\mbox{ }\mbox{ }$};
	\draw [black] (32.077,-17.44) arc (131.12228:99.93712:17.189);
	\fill [black] (32.08,-17.44) -- (33.01,-17.29) -- (32.35,-16.54);
	\draw (34.99,-14.37) node [above] {$b$};
	\draw [black] (44.959,-15.759) arc (26.66951:-1.27035:18.386);
	\fill [black] (46.91,-24.42) -- (47.43,-23.63) -- (46.43,-23.61);
	\draw (47.22,-19.58) node [right] {$c$};
	\draw [black] (46.395,-13.207) arc (83.9582:-116.6654:14.567);
	\fill [black] (38.32,-40.71) -- (38.81,-41.52) -- (39.26,-40.62);
	\draw (59.61,-32.38) node [right] {$d$};
	\draw [black] (44.57,-29.6) -- (37.83,-36.9);
	\fill [black] (37.83,-36.9) -- (38.74,-36.65) -- (38.01,-35.97);
	\draw (40.67,-31.79) node [left] {$d$};
	\draw [black] (43.687,-26.684) arc (-106.00985:-124.32589:39.144);
	\fill [black] (32.41,-21.38) -- (32.79,-22.25) -- (33.35,-21.42);
	\draw (36.85,-24.99) node [below] {$b$};
	\draw [black] (47.797,-30.145) arc (17.84688:-163.44093:15.041);
	\fill [black] (19.06,-39.04) -- (18.81,-39.95) -- (19.77,-39.67);
	\draw (38.73,-49.67) node [below] {$c$};
	\draw [black] (21.796,-6.766) arc (132.94565:-21.89007:16.264);
	\fill [black] (21.8,-6.77) -- (22.72,-6.59) -- (22.04,-5.85);
	\draw (43.03,-4.76) node [above] {$a$};
	\draw [black] (33.12,-40.423) arc (324:36:2.25);
	\draw (28.55,-39.1) node [left] {$d$};
	\fill [black] (33.12,-37.78) -- (32.77,-36.9) -- (32.18,-37.71);
	\draw [black] (36.754,-41.934) arc (10.72301:-210.39871:10.898);
	\fill [black] (16.65,-38.45) -- (15.81,-38.88) -- (16.67,-39.39);
	\draw (23.75,-55.29) node [below] {$c$};
	\draw [black] (34.94,-36.22) -- (30.86,-22.48);
	\fill [black] (30.86,-22.48) -- (30.6,-23.38) -- (31.56,-23.1);
	\draw (33.67,-28.75) node [right] {$b$};
	\draw [black] (21.619,-6.618) arc (138.23159:-82.24481:19.498);
	\fill [black] (21.62,-6.62) -- (22.52,-6.35) -- (21.78,-5.69);
	\draw (54.06,-9.29) node [right] {$a$};
	\end{tikzpicture}
\end{center}
	\caption[]{$ Q=\{0,1,2,3,4\}$       $m=1$}
\end{figure}





\begin{figure}[ht]
\begin{center}
	\begin{tikzpicture}[scale=0.2]
	\tikzstyle{every node}+=[inner sep=0pt]
	\draw [black] (4.6,-12.2) circle (3);
	\draw (4.6,-12.2) node {$1$};
	\draw [black] (30.6,-24.2) circle (3);
	\draw (30.6,-24.2) node {$2$};
	\draw [black] (6.2,-48.3) circle (3);
	\draw (6.2,-48.3) node {$3$};
	\draw [black] (49.8,-49.5) circle (3);
	\draw (49.8,-49.5) node {$4$};
	\draw [black] (55.9,-12.2) circle (3);
	\draw (55.9,-12.2) node {$5$};
	\draw [black] (55.9,-34.2) circle (3);
	\draw (55.9,-34.2) node {$6$};
	\draw [black] (55.9,-34.2) circle (2.4);
	\draw [black] (3.277,-9.52) arc (234:-54:2.25);
	\draw (4.6,-4.95) node [above] {$p(a|a)$};
	\fill [black] (5.92,-9.52) -- (6.8,-9.17) -- (5.99,-8.58);
	\draw [black] (30.106,-21.253) arc (217.25487:-70.74513:2.25);
	\draw (34.95,-17.1) node [above] {$p(b|b)$};
	\fill [black] (32.64,-22.01) -- (33.58,-21.93) -- (32.97,-21.13);
	\draw [black] (6.328,-51.286) arc (30.18097:-257.81903:2.25);
	\draw (0.79,-55) node [below] {$p(c|c)$};
	\fill [black] (3.91,-50.22) -- (2.97,-50.19) -- (3.47,-51.05);
	\draw [black] (5.714,-45.34) arc (-171.29151:-183.63296:140.372);
	\fill [black] (5.71,-45.34) -- (6.09,-44.47) -- (5.1,-44.62);
	\draw (3.67,-30.32) node [left] {$\mbox{ }\mbox{ }\mbox{ }\mbox{ }\mbox{ }\mbox{ }\mbox{ }\mbox{ }p(c|a)$};
	\draw [black] (7.079,-10.512) arc (122.2697:57.7303:43.399);
	\fill [black] (53.42,-10.51) -- (53.01,-9.66) -- (52.48,-10.51);
	\draw (30.25,-3.31) node [above] {$p(d|a)$};
	\draw [black] (7.583,-12.513) arc (81.8726:48.57712:40.066);
	\fill [black] (7.58,-12.51) -- (8.3,-13.12) -- (8.45,-12.13);
	\draw (21.47,-15.28) node [above] {$p(a|b)$};
	\draw [black] (7.532,-45.612) arc (151.86818:117.42301:48.336);
	\fill [black] (7.53,-45.61) -- (8.35,-45.14) -- (7.47,-44.67);
	\draw (13.36,-33.53) node [above] {$p(c|b)$};
	\draw [black] (47.431,-47.66) arc (-129.36992:-156.24084:55.875);
	\fill [black] (47.43,-47.66) -- (47.13,-46.77) -- (46.5,-47.54);
	\draw (37.79,-39.65) node [left] {$\mbox{ }\mbox{ }p(d|b)$};
	\draw [black] (46.917,-50.329) arc (-75.2387:-107.9144:67.363);
	\fill [black] (46.92,-50.33) -- (46.02,-50.05) -- (46.27,-51.02);
	\draw (27.86,-53.1) node [below] {$p(d|c)$};
	\draw [black] (5.432,-15.082) arc (14.88666:-9.81113:70.847);
	\fill [black] (5.43,-15.08) -- (5.15,-15.98) -- (6.12,-15.73);
	\draw (8.31,-30.13) node [right] {$p(a|c)$};
	\draw [black] (27.764,-23.221) arc (-109.71785:-119.83243:128.561);
	\fill [black] (27.76,-23.22) -- (27.18,-22.48) -- (26.84,-23.42);
	\draw (14.5,-19.45) node [below] {$p(b|a)$};
	\draw [black] (31.568,-27.036) arc (14.17669:-104.88549:18.382);
	\fill [black] (31.57,-27.04) -- (31.28,-27.93) -- (32.25,-27.69);
	\draw (29.5,-45.09) node [below] {$p(b|c)$};
	\draw [black] (7.592,-11.979) arc (93.95767:86.04233:328.286);
	\fill [black] (7.59,-11.98) -- (8.42,-12.42) -- (8.36,-11.43);
	\draw (30.25,-10.7) node [above] {$p(a|d)$};
	\draw [black] (53.622,-14.151) arc (-51.2137:-78.03555:47.886);
	\fill [black] (33.55,-23.67) -- (34.44,-23.99) -- (34.23,-23.02);
	\draw (46.97,-20.61) node [below] {$p(b|d)$};
	\draw [black] (55.607,-31.214) arc (-175.27936:-184.72064:97.384);
	\fill [black] (55.61,-31.21) -- (56.04,-30.38) -- (55.04,-30.46);
	\draw (54.78,-23.2) node [left] {$p(c|d)$};
	\draw [black] (58.099,-14.237) arc (43.2333:-61.80904:21.727);
	\fill [black] (52.53,-48.27) -- (53.47,-48.33) -- (53,-47.45);
	\draw (64.42,-32.85) node [right] {$p(d|d)$};
	\draw [black] (52.291,-51.151) arc (84.18903:-203.81097:2.25);
	\draw (56.05,-55.84) node [below] {$p(d|d)$};
	\fill [black] (50,-52.48) -- (49.43,-53.23) -- (50.42,-53.33);
	\draw [black] (47.55,-51.483) arc (-51.43786:-131.71524:30.426);
	\fill [black] (8.34,-50.4) -- (8.6,-51.31) -- (9.27,-50.56);
	\draw (27.7,-58.68) node [below] {$p(c|d)$};
	\draw [black] (32.939,-26.078) arc (49.80232:24.58692:59.422);
	\fill [black] (32.94,-26.08) -- (33.23,-26.98) -- (33.87,-26.21);
	\draw (42.5,-34.14) node [right] {$p(b|d)$};
	\draw [black] (46.949,-48.567) arc (-109.23723:-149.82312:76.434);
	\fill [black] (6.06,-14.82) -- (6.03,-15.77) -- (6.89,-15.26);
	\draw (20.67,-35.85) node [below] {$p(a|d)$};
	\draw [black] (57.526,-36.707) arc (60.70628:-227.29372:2.25);
	\draw (56.94,-41.49) node [below] {$1$};
	\fill [black] (54.9,-37.02) -- (54.07,-37.47) -- (54.94,-37.96);
	\end{tikzpicture}
\end{center}
	\caption[]{$ Q=\{0,1,2,3,4\}$       $m=1$}
\end{figure}